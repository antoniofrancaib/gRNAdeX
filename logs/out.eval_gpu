/home/jaf98/miniforge3/envs/rna/lib/python3.10/site-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.

Due to the on going maintenance burden of keeping command line application
wrappers up to date, we have decided to deprecate and eventually remove these
modules.

We instead now recommend building your command line and invoking it directly
with the subprocess module.
  warnings.warn(
wandb: Currently logged in as: jaf98 (jaf98-university-of-cambridge) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /rds/user/jaf98/hpc-work/geometric-rna-design/wandb/wandb/run-20250325_111832-70nmd8a3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rna_eval_filt_baseline
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jaf98-university-of-cambridge/gRNAde
wandb: üöÄ View run at https://wandb.ai/jaf98-university-of-cambridge/gRNAde/runs/70nmd8a3

CONFIG
    device: gpu
    gpu: 0
    seed: 0
    save: True
    data_path: /rds/user/jaf98/hpc-work/geometric-rna-design/data/
    radius: 4.5
    top_k: 32
    num_rbf: 32
    num_posenc: 32
    max_num_conformers: 1
    noise_scale: 0.1
    max_nodes_batch: 3000
    max_nodes_sample: 5000
    split: das
    model: ARv2
    node_in_dim: [15, 4]
    node_h_dim: [128, 16]
    edge_in_dim: [131, 3]
    edge_h_dim: [64, 4]
    num_layers: 4
    drop_rate: 0.5
    out_dim: 4
    attention_heads: 4
    attention_dropout: 0.1
    epochs: 50
    lr: 0.0001
    label_smoothing: 0.05
    batch_size: 8
    num_workers: 8
    val_every: 5
    model_path: /home/jaf98/rds/hpc-work/geometric-rna-design/checkpoints/gRNAde_ARv1_das.h5
    evaluate: True
    n_samples: 16
    temperature: 0.1

MODEL
    AutoregressiveMultiGNNv2(
  (W_v): Sequential(
    (0): LayerNorm(
      (scalar_norm): LayerNorm((15,), eps=1e-05, elementwise_affine=True)
    )
    (1): GVP(
      (wh): Linear(in_features=4, out_features=16, bias=False)
      (ws): Linear(in_features=31, out_features=128, bias=True)
      (wv): Linear(in_features=16, out_features=16, bias=False)
      (wsv): Linear(in_features=128, out_features=16, bias=True)
    )
  )
  (W_e): Sequential(
    (0): LayerNorm(
      (scalar_norm): LayerNorm((131,), eps=1e-05, elementwise_affine=True)
    )
    (1): GVP(
      (wh): Linear(in_features=3, out_features=4, bias=False)
      (ws): Linear(in_features=135, out_features=64, bias=True)
      (wv): Linear(in_features=4, out_features=4, bias=False)
      (wsv): Linear(in_features=64, out_features=4, bias=True)
    )
  )
  (encoder_layers): ModuleList(
    (0-3): 4 x MultiAttentiveGVPLayer(
      (conv): MultiGVPConv()
      (attention): GraphAttentionLayer(
        (W_Q): Linear(in_features=144, out_features=576, bias=True)
        (W_K): Linear(in_features=144, out_features=576, bias=True)
        (W_V): Linear(in_features=128, out_features=512, bias=True)
        (W_O): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (norm): LayerNorm(
        (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (dropout): Dropout(
        (sdropout): Dropout(p=0.5, inplace=False)
        (vdropout): _VDropout()
      )
    )
  )
  (psi): Sequential(
    (0): Linear(in_features=16512, out_features=256, bias=True)
    (1): SiLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=256, out_features=128, bias=True)
  )
  (W_s): Embedding(4, 4)
  (decoder_layers): ModuleList(
    (0-3): 4 x AttentiveGVPLayer(
      (gvp_branch): GVPConvLayer(
        (conv): GVPConv()
        (norm): ModuleList(
          (0-1): 2 x LayerNorm(
            (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
        )
        (dropout): ModuleList(
          (0-1): 2 x Dropout(
            (sdropout): Dropout(p=0.5, inplace=False)
            (vdropout): _VDropout()
          )
        )
        (ff_func): Sequential(
          (0): GVP(
            (wh): Linear(in_features=16, out_features=32, bias=False)
            (ws): Linear(in_features=160, out_features=512, bias=True)
            (wv): Linear(in_features=32, out_features=32, bias=False)
            (wsv): Linear(in_features=512, out_features=32, bias=True)
          )
          (1): GVP(
            (wh): Linear(in_features=32, out_features=32, bias=False)
            (ws): Linear(in_features=544, out_features=128, bias=True)
            (wv): Linear(in_features=32, out_features=16, bias=False)
            (wsv): Linear(in_features=128, out_features=16, bias=True)
          )
        )
      )
      (attention_branch): GraphAttentionLayer(
        (W_Q): Linear(in_features=144, out_features=576, bias=True)
        (W_K): Linear(in_features=144, out_features=576, bias=True)
        (W_V): Linear(in_features=128, out_features=512, bias=True)
        (W_O): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (norm): LayerNorm(
        (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (dropout): Dropout(
        (sdropout): Dropout(p=0.5, inplace=False)
        (vdropout): _VDropout()
      )
      (ff_func): Sequential(
        (0): GVP(
          (wh): Linear(in_features=16, out_features=32, bias=False)
          (ws): Linear(in_features=160, out_features=512, bias=True)
          (wv): Linear(in_features=32, out_features=32, bias=False)
          (wsv): Linear(in_features=512, out_features=32, bias=True)
        )
        (1): GVP(
          (wh): Linear(in_features=32, out_features=32, bias=False)
          (ws): Linear(in_features=544, out_features=128, bias=True)
          (wv): Linear(in_features=32, out_features=16, bias=False)
          (wsv): Linear(in_features=128, out_features=16, bias=True)
        )
      )
      (final_norm): LayerNorm(
        (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (W_out): GVP(
    (wh): Linear(in_features=16, out_features=16, bias=False)
    (ws): Linear(in_features=144, out_features=4, bias=True)
  )
)
    Total parameters: 8405992

TEST DATASET
    Pre-processing 98 samples
  0%|          | 0/98 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [00:00<00:00, 2706.72it/s]
    Finished: 98 pre-processed samples
Loading RhoFold checkpoint: /rds/user/jaf98/hpc-work/geometric-rna-design/tools/rhofold/model_20221010_params.pt
  0%|          | 0/98 [00:00<?, ?it/s]  1%|          | 1/98 [00:22<36:39, 22.68s/it]  2%|‚ñè         | 2/98 [00:44<35:53, 22.44s/it]  3%|‚ñé         | 3/98 [01:06<34:57, 22.08s/it]  4%|‚ñç         | 4/98 [01:28<34:21, 21.93s/it]  5%|‚ñå         | 5/98 [01:49<33:39, 21.72s/it]  6%|‚ñå         | 6/98 [02:11<33:17, 21.71s/it]  7%|‚ñã         | 7/98 [02:32<32:32, 21.46s/it]  8%|‚ñä         | 8/98 [02:51<31:17, 20.86s/it]  9%|‚ñâ         | 9/98 [03:13<31:21, 21.14s/it] 10%|‚ñà         | 10/98 [03:35<31:16, 21.32s/it] 11%|‚ñà         | 11/98 [03:56<30:45, 21.21s/it] 12%|‚ñà‚ñè        | 12/98 [04:17<30:28, 21.27s/it] 13%|‚ñà‚ñé        | 13/98 [04:36<28:56, 20.43s/it] 14%|‚ñà‚ñç        | 14/98 [04:57<29:05, 20.78s/it] 15%|‚ñà‚ñå        | 15/98 [05:19<29:05, 21.03s/it] 16%|‚ñà‚ñã        | 16/98 [05:40<28:57, 21.19s/it] 17%|‚ñà‚ñã        | 17/98 [06:02<28:38, 21.21s/it] 18%|‚ñà‚ñä        | 18/98 [06:25<29:12, 21.91s/it] 19%|‚ñà‚ñâ        | 19/98 [06:49<29:30, 22.41s/it] 20%|‚ñà‚ñà        | 20/98 [07:12<29:31, 22.72s/it] 21%|‚ñà‚ñà‚ñè       | 21/98 [07:36<29:22, 22.89s/it] 22%|‚ñà‚ñà‚ñè       | 22/98 [08:00<29:31, 23.31s/it] 23%|‚ñà‚ñà‚ñé       | 23/98 [08:24<29:34, 23.66s/it] 24%|‚ñà‚ñà‚ñç       | 24/98 [08:48<29:06, 23.60s/it] 26%|‚ñà‚ñà‚ñå       | 25/98 [09:12<28:47, 23.67s/it] 27%|‚ñà‚ñà‚ñã       | 26/98 [09:35<28:27, 23.71s/it] 28%|‚ñà‚ñà‚ñä       | 27/98 [09:59<28:02, 23.70s/it] 29%|‚ñà‚ñà‚ñä       | 28/98 [10:23<27:33, 23.62s/it] 30%|‚ñà‚ñà‚ñâ       | 29/98 [10:46<27:03, 23.52s/it] 31%|‚ñà‚ñà‚ñà       | 30/98 [11:09<26:30, 23.38s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 31/98 [11:32<26:11, 23.45s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 32/98 [11:55<25:36, 23.28s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 33/98 [12:19<25:20, 23.39s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 34/98 [12:42<24:53, 23.33s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 35/98 [13:06<24:29, 23.33s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 36/98 [13:29<24:07, 23.35s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 37/98 [13:53<23:56, 23.55s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 38/98 [14:16<23:26, 23.44s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 39/98 [14:40<23:10, 23.56s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 40/98 [15:03<22:45, 23.54s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 41/98 [15:27<22:18, 23.48s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 42/98 [15:51<22:00, 23.58s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 43/98 [16:13<21:22, 23.32s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/98 [16:37<21:02, 23.38s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/98 [17:00<20:33, 23.27s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 46/98 [17:23<20:03, 23.14s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 47/98 [17:47<19:56, 23.46s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 48/98 [18:10<19:28, 23.37s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 49/98 [18:32<18:49, 23.06s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/98 [18:56<18:29, 23.11s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 51/98 [19:19<18:08, 23.16s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 52/98 [19:44<18:10, 23.71s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 53/98 [20:09<18:06, 24.14s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 54/98 [20:34<17:57, 24.49s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/98 [20:59<17:39, 24.64s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 56/98 [21:24<17:21, 24.79s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 57/98 [21:49<16:53, 24.71s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 58/98 [22:12<16:08, 24.20s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 59/98 [22:38<16:03, 24.71s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/98 [23:02<15:29, 24.47s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 61/98 [23:26<15:02, 24.39s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 62/98 [23:51<14:40, 24.46s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 63/98 [24:08<13:05, 22.44s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 64/98 [24:26<11:54, 21.03s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 65/98 [25:06<14:38, 26.62s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 66/98 [25:56<17:57, 33.67s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 67/98 [26:35<18:16, 35.37s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 68/98 [27:25<19:47, 39.59s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 69/98 [28:14<20:37, 42.66s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 70/98 [28:54<19:31, 41.83s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 71/98 [29:34<18:28, 41.05s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 72/98 [30:12<17:30, 40.39s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 73/98 [30:51<16:37, 39.91s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 74/98 [31:40<17:03, 42.64s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 75/98 [32:18<15:49, 41.26s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 76/98 [32:37<12:38, 34.48s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 77/98 [32:55<10:19, 29.48s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 78/98 [33:13<08:39, 25.98s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 79/98 [34:13<11:32, 36.43s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 80/98 [35:14<13:05, 43.62s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 81/98 [36:14<13:46, 48.64s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 82/98 [37:14<13:50, 51.89s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 83/98 [38:14<13:35, 54.37s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 84/98 [39:16<13:14, 56.75s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 85/98 [40:20<12:43, 58.75s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 86/98 [41:21<11:53, 59.49s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 87/98 [42:21<10:56, 59.71s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 88/98 [43:21<09:58, 59.81s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 89/98 [43:42<07:12, 48.08s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 90/98 [44:03<05:20, 40.04s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 91/98 [44:24<04:00, 34.39s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 92/98 [44:45<03:01, 30.22s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 93/98 [45:06<02:18, 27.65s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 94/98 [45:24<01:39, 24.76s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 95/98 [45:42<01:08, 22.77s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 96/98 [46:01<00:42, 21.42s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 97/98 [46:21<00:21, 21.01s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [46:38<00:00, 20.01s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [46:38<00:00, 28.56s/it]
scscore_rmsd: 12.4674                scscore_tm: 0.2304                scscore_gdt: 0.2364                rmsd_within_thresh: 0.0217                tm_within_thresh: 0.2054                gdt_within_thresh: 0.1837
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mrna_eval_filt_baseline[0m at: [34mhttps://wandb.ai/jaf98-university-of-cambridge/gRNAde/runs/70nmd8a3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/wandb/run-20250325_111832-70nmd8a3/logs[0m
